\documentclass[11pt]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}

% Page setup
\geometry{margin=1in}

% Title and author information
\title{Small Language Reasoning Models}
\author{Pablo Leyva, Xiaoning Ding\\
\small pl33@njit.edu, xiaoning.ding@njit.edu}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
    % Abstract content goes here

    A rise of Small Language Models (SLMs) has been seen in the last few years; however, these models are being used as a general purpose models for a wide range of tasks such as text generation, translation, and summarization.
    These models became more popular due to usability and accessibility in limited hardware resource use cases. We propose a new class of foundational models that are trained to reason and problem solve one step at a time.
    Small Language Reasoning Models (SLRMs) are not meant for general chatbot applications but rather for internal reasoning and planning in agentic workflows.

\end{abstract}

\section{Introduction}

% Introduction content goes here

\section{Objectives}

% Objectives content goes here

\section{Significance}

\subsection{Content}
% Content subsection goes here

\subsection{Innovations}
% Innovations subsection goes here

\section{Methodology}

% Methodology content goes here

\section{Expected Results}

\subsection{Outcomes}
% Outcomes content goes here

\subsection{Challenges}
% Challenges content goes here

\subsection{Mitigation}
% Mitigation content goes here

\section{Timeline}

% Timeline content goes here




\newpage

\begin{thebibliography}{9}

    \bibitem{belcak2024slm}
    Peter Belcak, Greg Heinrich, Shizhe Diao, Yonggan Fu, Xin Dong, Saurav Muralidharan, Yingyan Celine Lin, Pavlo Molchanov.\\
    \textit{Small Language Models are the Future of Agentic AI}.\\
    \href{https://arxiv.org/pdf/2506.02153}{arXiv:2506.02153}, 2024.

    \bibitem{meyerson2025millionstep}
    Elliot Meyerson, Giuseppe Paolo, Roberto Dailey, Hormoz Shahrzad, Olivier Francon, Conor F. Hayes, Xin Qiu, Babak Hodjat, Risto Miikkulainen.\\
    \textit{Solving a Million-Step LLM Task with Zero Errors}.\\
    \href{https://arxiv.org/pdf/2511.09030}{arXiv:2511.09030}, 2025.

    \bibitem{jolicoeurmartineau2025less}
    Alexia Jolicoeur-Martineau.\\
    \textit{Less is More: Recursive Reasoning with Tiny Networks}.\\
    \href{https://arxiv.org/pdf/2510.04871}{arXiv:2510.04871}, 2025.

    \bibitem{nexusraven2024}
    Nexusflow, Venkat Krishna Srinivasan, Zhen Dong, Banghua Zhu, Brian Yu, Damon Mosk-Aoyama, Kurt Keutzer, Jiantao Jiao, Jian Zhang.\\
    \textit{NexusRaven: A Commercially-Permissive Language Model for Function Calling}.\\
    \href{https://openreview.net/pdf?id=5lcPe6DqfI}{openreview.net}, 2024.

    \bibitem{patil2023gorilla}
    Shishir G. Patil, Tianjun Zhang, Xin Wang, Joseph E. Gonzalez.\\
    \textit{Gorilla: Large Language Model Connected with Massive APIs}.\\
    \href{https://arxiv.org/pdf/2305.15334}{arXiv:2305.15334}, 2023.


\end{thebibliography}


\end{document}

